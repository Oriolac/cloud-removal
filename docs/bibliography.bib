######### CLOUD DETECTION

@Article{Mahajan2020,
	author={Mahajan, Seema
	and Fataniya, Bhavin},
	title={Cloud detection methodologies: variants and development---a review},
	journal={Complex {\&} Intelligent Systems},
	year={2020},
	month={Jul},
	day={01},
	volume={6},
	number={2},
	pages={251-261},
	issn={2198-6053},
	doi={10.1007/s40747-019-00128-0},
	url={https://doi.org/10.1007/s40747-019-00128-0}
}
@COMMENT https://link.springer.com/article/10.1007/s40747-019-00128-0

@COMNENT Understanding clouds https://arxiv.org/abs/1906.01906

######### SURVEYS

######### AI State-of-the-art

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

@inproceedings{goodfellow2014generative,
	title={Generative adversarial nets},
	author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle={Advances in neural information processing systems},
	pages={2672--2680},
	year={2014}
}

@inproceedings{VaswaniSPUJGKP17,
	title = {Attention is All you Need},
	author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year = {2017},
	url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need},
	researchr = {https://researchr.org/publication/VaswaniSPUJGKP17},
	cites = {0},
	citedby = {0},
	pages = {6000-6010},
	booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA},
	editor = {Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and S. V. N. Vishwanathan and Roman Garnett},
}

@misc{brown2020language,
	added-at = {2021-03-29T17:49:32.000+0200},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	biburl = {https://www.bibsonomy.org/bibtex/295850dc02fa8a64a08252dc580322d39/frankyanpan},
	description = {2005.14165.pdf},
	interhash = {ca061f6d64797209bea0e0fdbc2f7af8},
	intrahash = {95850dc02fa8a64a08252dc580322d39},
	keywords = {GPT3 few-shotting},
	note = {cite arxiv:2005.14165Comment: 40+32 pages},
	timestamp = {2021-03-29T17:49:32.000+0200},
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	year = 2020
}

@article{pix2pix2017,
	title={Image-to-Image Translation with Conditional Adversarial Networks},
	author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
	journal={CVPR},
	year={2017}
}


@misc{unet,
	abstract = {There is large consent that successful training of deep networks requires
	many thousand annotated training samples. In this paper, we present a network
	and training strategy that relies on the strong use of data augmentation to use
	the available annotated samples more efficiently. The architecture consists of
	a contracting path to capture context and a symmetric expanding path that
	enables precise localization. We show that such a network can be trained
	end-to-end from very few images and outperforms the prior best method (a
	sliding-window convolutional network) on the ISBI challenge for segmentation of
	neuronal structures in electron microscopic stacks. Using the same network
	trained on transmitted light microscopy images (phase contrast and DIC) we won
	the ISBI cell tracking challenge 2015 in these categories by a large margin.
	Moreover, the network is fast. Segmentation of a 512x512 image takes less than
	a second on a recent GPU. The full implementation (based on Caffe) and the
	trained networks are available at
	http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	added-at = {2022-06-07T20:27:10.000+0200},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	biburl = {https://www.bibsonomy.org/bibtex/2a8a0c515657686d130c8cb297f7364fb/bradleybossard},
	description = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	interhash = {9158de16b2caff7458df054dc6fc2748},
	intrahash = {a8a0c515657686d130c8cb297f7364fb},
	keywords = {unet},
	note = {cite arxiv:1505.04597Comment: conditionally accepted at MICCAI 2015},
	timestamp = {2022-06-07T20:27:10.000+0200},
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {http://arxiv.org/abs/1505.04597},
	year = 2015
}



@InProceedings{pmlr-v119-chen20s,
	title = 	 {Generative Pretraining From Pixels},
	author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
	booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
	pages = 	 {1691--1703},
	year = 	 {2020},
	editor = 	 {III, Hal Daumé and Singh, Aarti},
	volume = 	 {119},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {13--18 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
	url = 	 {https://proceedings.mlr.press/v119/chen20s.html},
}

@inproceedings{CycleGAN2017,
	title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
	author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
	booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
	year={2017}
}


@InProceedings{pmlr-v139-ramesh21a,
	title = 	 {Zero-Shot Text-to-Image Generation},
	author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
	pages = 	 {8821--8831},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {18--24 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
	url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
	abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}




######### DATASETS
@article{sen12mscr,
	title = {{Multisensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery}},
	author = {Ebel, Patrick and Meraner, Andrea and Schmitt, Michael and Zhu, Xiao Xiang},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	year = {2020},
	publisher = {IEEE}
}
@article{sen12mscrts,
	title = {{SEN12MS-CR-TS: A Remote Sensing Data Set for Multi-modal Multi-temporal Cloud Removal}},
	author = {Ebel, Patrick and Xu, Yajin and Schmitt, Michael and Zhu, Xiao Xiang},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	year = {2022},
	publisher = {IEEE}
} 
@COMMENT{El millo dataset, Sentinel2!}

######### PROPOSALS

@INPROCEEDINGS{8014931,  author={Enomoto, Kenji and Sakurada, Ken and Wang, Weimin and Fukui, Hiroshi and Matsuoka, Masashi and Nakamura, Ryosuke and Kawaguchi, Nobuo},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},   title={Filmy Cloud Removal on Satellite Imagery with Multispectral Conditional Generative Adversarial Nets},   year={2017},  volume={},  number={},  pages={1533-1541},  doi={10.1109/CVPRW.2017.197}}

@INPROCEEDINGS{cloud-gan, 
	author={Singh, Praveer and Komodakis, Nikos},
	booktitle={IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium},
	title={Cloud-Gan: Cloud Removal for Sentinel-2 Imagery Using a Cyclic Consistent Generative Adversarial Networks},
	year={2018},
	volume={},
	number={},
	pages={1772-1775},
	doi={10.1109/IGARSS.2018.8519033}}
@COMMENT https://hal-enpc.archives-ouvertes.fr/hal-01832797/document

@article{LANARAS2018305,
	title = {Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	volume = {146},
	pages = {305-319},
	year = {2018},
	issn = {0924-2716},
	doi = {https://doi.org/10.1016/j.isprsjprs.2018.09.018},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271618302636},
	author = {Charis Lanaras and José Bioucas-Dias and Silvano Galliani and Emmanuel Baltsavias and Konrad Schindler},
	keywords = {Sentinel-2, Super-resolution, Sharpening of bands, Convolutional neural network, Deep learning},
	abstract = {The Sentinel-2 satellite mission delivers multi-spectral imagery with 13 spectral bands, acquired at three different spatial resolutions. The aim of this research is to super-resolve the lower-resolution (20 m and 60 m Ground Sampling Distance – GSD) bands to 10 m GSD, so as to obtain a complete data cube at the maximal sensor resolution. We employ a state-of-the-art convolutional neural network (CNN) to perform end-to-end upsampling, which is trained with data at lower resolution, i.e., from 40 → 20 m, respectively 360 → 60 m GSD. In this way, one has access to a virtually infinite amount of training data, by downsampling real Sentinel-2 images. We use data sampled globally over a wide range of geographical locations, to obtain a network that generalises across different climate zones and land-cover types, and can super-resolve arbitrary Sentinel-2 images without the need of retraining. In quantitative evaluations (at lower scale, where ground truth is available), our network, which we call DSen2, outperforms the best competing approach by almost 50% in RMSE, while better preserving the spectral characteristics. It also delivers visually convincing results at the full 10 m GSD.}
}

@COMMENT https://www.sciencedirect.com/science/article/pii/S0924271618302636

@article{Meraner2020,
	title = "Cloud removal in Sentinel-2 imagery using a deep residual neural network and SAR-optical data fusion",
	journal = "ISPRS Journal of Photogrammetry and Remote Sensing",	
	volume = "166",
	pages = "333 - 346",
	year = "2020",
	issn = "0924-2716",
	doi = "https://doi.org/10.1016/j.isprsjprs.2020.05.013",
	url = "http://www.sciencedirect.com/science/article/pii/S0924271620301398",
	author = "Andrea Meraner and Patrick Ebel and Xiao Xiang Zhu and Michael Schmitt",
	keywords = "Cloud removal, Optical imagery, SAR-optical, Data fusion, Deep learning, Residual network",
}
@COMMENT https://github.com/ameraner/dsen2-cr


@Article{rs14061342,
	AUTHOR = {Czerkawski, Mikolaj and Upadhyay, Priti and Davison, Christopher and Werkmeister, Astrid and Cardona, Javier and Atkinson, Robert and Michie, Craig and Andonovic, Ivan and Macdonald, Malcolm and Tachtatzis, Christos},
	TITLE = {Deep Internal Learning for Inpainting of Cloud-Affected Regions in Satellite Imagery},
	JOURNAL = {Remote Sensing},
	VOLUME = {14},
	YEAR = {2022},
	NUMBER = {6},
	ARTICLE-NUMBER = {1342},
	URL = {https://www.mdpi.com/2072-4292/14/6/1342},
	ISSN = {2072-4292},
	ABSTRACT = {Cloud cover remains a significant limitation to a broad range of applications relying on optical remote sensing imagery, including crop identification/yield prediction, climate monitoring, and land cover classification. A common approach to cloud removal treats the problem as an inpainting task and imputes optical data in the cloud-affected regions employing either mosaicing historical data or making use of sensing modalities not impacted by cloud obstructions, such as SAR. Recently, deep learning approaches have been explored in these applications; however, the majority of reported solutions rely on external learning practices, i.e., models trained on fixed datasets. Although these models perform well within the context of a particular dataset, a significant risk of spatial and temporal overfitting exists when applied in different locations or at different times. Here, cloud removal was implemented within an internal learning regime through an inpainting technique based on the deep image prior. The approach was evaluated on both a synthetic dataset with an exact ground truth, as well as real samples. The ability to inpaint the cloud-affected regions for varying weather conditions across a whole year with no prior training was demonstrated, and the performance of the approach was characterised.},
	DOI = {10.3390/rs14061342}
}
@COMMENT Multi-temporal dataset of india and scotland, great results. DSEN2-CR is cited. https://www.mdpi.com/2072-4292/14/6/1342/htm


@article{sarukkai2019cloud,
	title={Cloud Removal in Satellite Images Using Spatiotemporal Generative Networks},
	author={Sarukkai, Vishnu and Jain, Anirudh and Uzkent, Burak and Ermon, Stefano},
	journal={arXiv preprint arXiv:1912.06838},
	year={2019}
}